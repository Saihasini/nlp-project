# -*- coding: utf-8 -*-
"""distilbert_NER (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oiheZe133wC2A7EaA8G7j92DIJ8VZ5w7
"""

!unzip archive.zip

!pip install seqeval

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForTokenClassification,
    get_linear_schedule_with_warmup
)
from seqeval.metrics import (
    classification_report,
    f1_score as seq_f1_score,
    accuracy_score as seq_accuracy_score,
    precision_score as seq_precision_score,
    recall_score as seq_recall_score
)

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Define paths
data_dir = "conll2003"
train_path = os.path.join(data_dir, "eng.train")
val_path = os.path.join(data_dir, "eng.testa")
test_path = os.path.join(data_dir, "eng.testb")

# Function to read CoNLL-2003 data
def read_conll_file(file_path):
    sentences = []
    labels = []

    sentence = []
    label = []

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line.startswith("-DOCSTART-") or line == "":
                if len(sentence) > 0:
                    sentences.append(sentence)
                    labels.append(label)
                    sentence = []
                    label = []
            else:
                parts = line.split()
                if len(parts) >= 4:  # CoNLL-2003 format has word, POS, chunk, NER
                    sentence.append(parts[0])
                    label.append(parts[3])

    if len(sentence) > 0:
        sentences.append(sentence)
        labels.append(label)

    return sentences, labels

# Load data
train_sentences, train_labels = read_conll_file(train_path)
val_sentences, val_labels = read_conll_file(val_path)
test_sentences, test_labels = read_conll_file(test_path)

print(f"Number of training examples: {len(train_sentences)}")
print(f"Number of validation examples: {len(val_sentences)}")
print(f"Number of test examples: {len(test_sentences)}")

# Get unique labels and create label mapping
unique_labels = set()
for label_list in train_labels + val_labels + test_labels:
    unique_labels.update(label_list)

label_list = sorted(list(unique_labels))
label_map = {label: i for i, label in enumerate(label_list)}
num_labels = len(label_list)

print(f"Unique labels: {label_list}")
print(f"Number of labels: {num_labels}")

# Load tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-cased")

# Custom dataset class
class NERDataset(Dataset):
    def __init__(self, sentences, labels, tokenizer, label_map, max_length=128):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.label_map = label_map
        self.max_length = max_length

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        word_labels = self.labels[idx]

        # Tokenize the sentence
        encoding = self.tokenizer(
            sentence,
            is_split_into_words=True,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt'
        )

        # Flatten the tensors
        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        # Create token labels
        token_labels = torch.ones_like(input_ids) * -100  # -100 is the ignore index for CrossEntropyLoss

        # Map word labels to token labels
        word_ids = encoding.word_ids()
        previous_word_idx = None

        for i, word_idx in enumerate(word_ids):
            if word_idx is None or word_idx == previous_word_idx:
                # Special token or continuation of a word
                continue

            if word_idx < len(word_labels):
                token_labels[i] = self.label_map[word_labels[word_idx]]

            previous_word_idx = word_idx

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': token_labels
        }

# Create datasets
train_dataset = NERDataset(train_sentences, train_labels, tokenizer, label_map)
val_dataset = NERDataset(val_sentences, val_labels, tokenizer, label_map)
test_dataset = NERDataset(test_sentences, test_labels, tokenizer, label_map)

# Create data loaders
batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Load model
model = DistilBertForTokenClassification.from_pretrained(
    "distilbert-base-cased",
    num_labels=num_labels
)
model.to(device)

# Training parameters
epochs = 3
learning_rate = 5e-5
warmup_steps = 0
weight_decay = 0.01

# Optimizer and scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=warmup_steps,
    num_training_steps=total_steps
)

# Training function
def train_epoch(model, dataloader, optimizer, scheduler, device):
    model.train()
    total_loss = 0

    progress_bar = tqdm(dataloader, desc="Training")
    for batch in progress_bar:
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()

        progress_bar.set_postfix({'loss': loss.item()})

    return total_loss / len(dataloader)

# Evaluation function with comprehensive metrics
def evaluate(model, dataloader, device, label_list):
    model.eval()
    total_loss = 0

    all_true_labels = []
    all_pred_labels = []

    # For token-level metrics
    all_token_true = []
    all_token_pred = []

    for batch in tqdm(dataloader, desc="Evaluating"):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

        loss = outputs.loss
        total_loss += loss.item()

        # Get predictions
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=2)

        # Convert to list of labels for sequence evaluation
        true_labels = [[label_list[l.item()] for l in label if l != -100] for label in labels]
        pred_labels = []

        for i, pred in enumerate(predictions):
            pred_list = []
            for j, p in enumerate(pred):
                if labels[i][j] != -100:  # Only consider tokens that have a label
                    pred_list.append(label_list[p.item()])
                    # For token-level metrics
                    all_token_true.append(labels[i][j].item())
                    all_token_pred.append(p.item())
            pred_labels.append(pred_list)

        all_true_labels.extend(true_labels)
        all_pred_labels.extend(pred_labels)

    # Calculate sequence-level metrics (entity-level)
    seq_report = classification_report(all_true_labels, all_pred_labels, output_dict=True)
    seq_f1 = seq_f1_score(all_true_labels, all_pred_labels)
    seq_precision = seq_precision_score(all_true_labels, all_pred_labels)
    seq_recall = seq_recall_score(all_true_labels, all_pred_labels)
    seq_accuracy = seq_accuracy_score(all_true_labels, all_pred_labels)

    # Calculate token-level metrics
    token_precision, token_recall, token_f1, _ = precision_recall_fscore_support(
        all_token_true, all_token_pred, average='weighted'
    )
    token_accuracy = accuracy_score(all_token_true, all_token_pred)

    # Combine metrics
    metrics = {
        'loss': total_loss / len(dataloader),
        'entity_f1': seq_f1,
        'entity_precision': seq_precision,
        'entity_recall': seq_recall,
        'entity_accuracy': seq_accuracy,
        'token_f1': token_f1,
        'token_precision': token_precision,
        'token_recall': token_recall,
        'token_accuracy': token_accuracy,
        'report': seq_report
    }

    return metrics, all_true_labels, all_pred_labels

# Training loop with metrics tracking
train_losses = []
val_losses = []
val_metrics_history = {
    'entity_f1': [],
    'entity_precision': [],
    'entity_recall': [],
    'entity_accuracy': [],
    'token_f1': [],
    'token_precision': [],
    'token_recall': [],
    'token_accuracy': []
}

for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")

    # Train
    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)
    train_losses.append(train_loss)

    # Evaluate on validation set
    val_metrics, _, _ = evaluate(model, val_loader, device, label_list)
    val_losses.append(val_metrics['loss'])

    # Store metrics history
    for metric_name in val_metrics_history.keys():
        val_metrics_history[metric_name].append(val_metrics[metric_name])

    print(f"Train Loss: {train_loss:.4f}")
    print(f"Validation Loss: {val_metrics['loss']:.4f}")
    print(f"Entity-level metrics:")
    print(f"  F1: {val_metrics['entity_f1']:.4f}")
    print(f"  Precision: {val_metrics['entity_precision']:.4f}")
    print(f"  Recall: {val_metrics['entity_recall']:.4f}")
    print(f"  Accuracy: {val_metrics['entity_accuracy']:.4f}")
    print(f"Token-level metrics:")
    print(f"  F1: {val_metrics['token_f1']:.4f}")
    print(f"  Precision: {val_metrics['token_precision']:.4f}")
    print(f"  Recall: {val_metrics['token_recall']:.4f}")
    print(f"  Accuracy: {val_metrics['token_accuracy']:.4f}")
    print("---")

# Evaluate on test set
test_metrics, true_labels, pred_labels = evaluate(model, test_loader, device, label_list)

print("\nTest Results:")
print(f"Test Loss: {test_metrics['loss']:.4f}")
print(f"Entity-level metrics:")
print(f"  F1: {test_metrics['entity_f1']:.4f}")
print(f"  Precision: {test_metrics['entity_precision']:.4f}")
print(f"  Recall: {test_metrics['entity_recall']:.4f}")
print(f"  Accuracy: {test_metrics['entity_accuracy']:.4f}")
print(f"Token-level metrics:")
print(f"  F1: {test_metrics['token_f1']:.4f}")
print(f"  Precision: {test_metrics['token_precision']:.4f}")
print(f"  Recall: {test_metrics['token_recall']:.4f}")
print(f"  Accuracy: {test_metrics['token_accuracy']:.4f}")

# Print detailed classification report
print("\nDetailed Classification Report:")
print(classification_report(true_labels, pred_labels))

# Save the model
model_save_path = "distilbert_ner_model"
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)
print(f"Model saved to {model_save_path}")

# Create a DataFrame for metrics
metrics_df = pd.DataFrame({
    'Epoch': list(range(1, epochs + 1)),
    'Train Loss': train_losses,
    'Validation Loss': val_losses,
    'Entity F1': val_metrics_history['entity_f1'],
    'Entity Precision': val_metrics_history['entity_precision'],
    'Entity Recall': val_metrics_history['entity_recall'],
    'Entity Accuracy': val_metrics_history['entity_accuracy'],
    'Token F1': val_metrics_history['token_f1'],
    'Token Precision': val_metrics_history['token_precision'],
    'Token Recall': val_metrics_history['token_recall'],
    'Token Accuracy': val_metrics_history['token_accuracy']
})

# Save metrics to CSV
metrics_df.to_csv('training_metrics.csv', index=False)
print("Metrics saved to training_metrics.csv")

# Visualize metrics - Training and Validation Loss
plt.figure(figsize=(10, 6))
plt.plot(range(1, epochs + 1), train_losses, 'b-', label='Training Loss')
plt.plot(range(1, epochs + 1), val_losses, 'r-', label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curve.png')
plt.show()

# Visualize Entity-level metrics
plt.figure(figsize=(12, 8))
plt.plot(range(1, epochs + 1), val_metrics_history['entity_f1'], 'g-', label='F1 Score')
plt.plot(range(1, epochs + 1), val_metrics_history['entity_precision'], 'b-', label='Precision')
plt.plot(range(1, epochs + 1), val_metrics_history['entity_recall'], 'r-', label='Recall')
plt.plot(range(1, epochs + 1), val_metrics_history['entity_accuracy'], 'y-', label='Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.title('Entity-level Metrics')
plt.legend()
plt.grid(True)
plt.savefig('entity_metrics.png')
plt.show()

# Visualize Token-level metrics
plt.figure(figsize=(12, 8))
plt.plot(range(1, epochs + 1), val_metrics_history['token_f1'], 'g-', label='F1 Score')
plt.plot(range(1, epochs + 1), val_metrics_history['token_precision'], 'b-', label='Precision')
plt.plot(range(1, epochs + 1), val_metrics_history['token_recall'], 'r-', label='Recall')
plt.plot(range(1, epochs + 1), val_metrics_history['token_accuracy'], 'y-', label='Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.title('Token-level Metrics')
plt.legend()
plt.grid(True)
plt.savefig('token_metrics.png')
plt.show()

# Create confusion matrix for each entity type
def plot_confusion_matrices(true_labels, pred_labels, label_list):
    # Flatten the lists of labels
    flat_true = []
    flat_pred = []

    for doc_true, doc_pred in zip(true_labels, pred_labels):
        flat_true.extend(doc_true)
        flat_pred.extend(doc_pred)

    # Create confusion matrix
    labels = [label for label in label_list if label != 'O']  # Exclude 'O' for better visualization

    # Filter out 'O' labels for better visualization
    filtered_true = []
    filtered_pred = []

    for t, p in zip(flat_true, flat_pred):
        if t != 'O' or p != 'O':
            filtered_true.append(t)
            filtered_pred.append(p)

    cm = confusion_matrix(filtered_true, filtered_pred, labels=labels)

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix (excluding O tag)')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png')
    plt.show()

# Plot confusion matrices
plot_confusion_matrices(true_labels, pred_labels, label_list)

# Calculate and plot per-class F1 scores
def plot_per_class_metrics(report):
    # Extract per-class metrics
    classes = []
    f1_scores = []
    precisions = []
    recalls = []

    for label, metrics in report.items():
        if label not in ['micro avg', 'macro avg', 'weighted avg', 'accuracy']:
            classes.append(label)
            f1_scores.append(metrics['f1-score'])
            precisions.append(metrics['precision'])
            recalls.append(metrics['recall'])

    # Sort by F1 score
    sorted_indices = np.argsort(f1_scores)
    classes = [classes[i] for i in sorted_indices]
    f1_scores = [f1_scores[i] for i in sorted_indices]
    precisions = [precisions[i] for i in sorted_indices]
    recalls = [recalls[i] for i in sorted_indices]

    # Plot
    plt.figure(figsize=(12, 6))
    x = np.arange(len(classes))
    width = 0.25

    plt.bar(x - width, precisions, width, label='Precision')
    plt.bar(x, recalls, width, label='Recall')
    plt.bar(x + width, f1_scores, width, label='F1 Score')

    plt.xlabel('Entity Type')
    plt.ylabel('Score')
    plt.title('Per-class Metrics')
    plt.xticks(x, classes, rotation=45, ha='right')
    plt.legend()
    plt.tight_layout()
    plt.savefig('per_class_metrics.png')
    plt.show()

# Plot per-class metrics
plot_per_class_metrics(test_metrics['report'])

# Create a summary table of all metrics
def create_metrics_summary_table(test_metrics):
    # Entity-level metrics
    entity_metrics = {
        'Metric': ['F1 Score', 'Precision', 'Recall', 'Accuracy'],
        'Entity-level': [
            test_metrics['entity_f1'],
            test_metrics['entity_precision'],
            test_metrics['entity_recall'],
            test_metrics['entity_accuracy']
        ],
        'Token-level': [
            test_metrics['token_f1'],
            test_metrics['token_precision'],
            test_metrics['token_recall'],
            test_metrics['token_accuracy']
        ]
    }

    # Create DataFrame
    metrics_df = pd.DataFrame(entity_metrics)

    # Format as percentages
        # Format as percentages
    metrics_df['Entity-level'] = metrics_df['Entity-level'].apply(lambda x: f"{x:.2%}")
    metrics_df['Token-level'] = metrics_df['Token-level'].apply(lambda x: f"{x:.2%}")

    return metrics_df

# Display and save metrics summary
metrics_summary = create_metrics_summary_table(test_metrics)
print("\nMetrics Summary:")
print(metrics_summary)
metrics_summary.to_csv('metrics_summary.csv', index=False)

# Create a detailed per-class metrics table
def create_per_class_metrics_table(report):
    # Extract per-class metrics
    classes = []
    f1_scores = []
    precisions = []
    recalls = []
    supports = []

    for label, metrics in report.items():
        if label not in ['micro avg', 'macro avg', 'weighted avg', 'accuracy']:
            classes.append(label)
            f1_scores.append(metrics['f1-score'])
            precisions.append(metrics['precision'])
            recalls.append(metrics['recall'])
            supports.append(metrics['support'])

    # Create DataFrame
    per_class_df = pd.DataFrame({
        'Entity Type': classes,
        'Precision': precisions,
        'Recall': recalls,
        'F1 Score': f1_scores,
        'Support': supports
    })

    # Sort by F1 score descending
    per_class_df = per_class_df.sort_values('F1 Score', ascending=False)

    # Format as percentages (except Support)
    per_class_df['Precision'] = per_class_df['Precision'].apply(lambda x: f"{x:.2%}")
    per_class_df['Recall'] = per_class_df['Recall'].apply(lambda x: f"{x:.2%}")
    per_class_df['F1 Score'] = per_class_df['F1 Score'].apply(lambda x: f"{x:.2%}")

    return per_class_df

# Display and save per-class metrics
per_class_metrics = create_per_class_metrics_table(test_metrics['report'])
print("\nPer-class Metrics:")
print(per_class_metrics)
per_class_metrics.to_csv('per_class_metrics.csv', index=False)

# Create a visual representation of the metrics summary
plt.figure(figsize=(10, 6))
metrics_data = {
    'Entity F1': test_metrics['entity_f1'],
    'Entity Precision': test_metrics['entity_precision'],
    'Entity Recall': test_metrics['entity_recall'],
    'Entity Accuracy': test_metrics['entity_accuracy'],
    'Token F1': test_metrics['token_f1'],
    'Token Precision': test_metrics['token_precision'],
    'Token Recall': test_metrics['token_recall'],
    'Token Accuracy': test_metrics['token_accuracy']
}

# Create bar chart
plt.bar(metrics_data.keys(), metrics_data.values(), color=['blue', 'green', 'red', 'purple', 'orange', 'cyan', 'magenta', 'yellow'])
plt.ylim(0, 1.0)
plt.ylabel('Score')
plt.title('Model Performance Metrics')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('metrics_summary.png')
plt.show()

# Create a radar chart for a different visualization of metrics
def plot_radar_chart(metrics):
    categories = ['F1 Score', 'Precision', 'Recall', 'Accuracy']
    entity_values = [
        metrics['entity_f1'],
        metrics['entity_precision'],
        metrics['entity_recall'],
        metrics['entity_accuracy']
    ]
    token_values = [
        metrics['token_f1'],
        metrics['token_precision'],
        metrics['token_recall'],
        metrics['token_accuracy']
    ]

    # Number of variables
    N = len(categories)

    # What will be the angle of each axis in the plot (divide the plot / number of variables)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]  # Close the loop

    # Add the first point at the end to close the polygon
    entity_values += entity_values[:1]
    token_values += token_values[:1]

    # Initialize the figure
    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))

    # Draw one axis per variable and add labels
    plt.xticks(angles[:-1], categories, size=12)

    # Draw the entity level metrics
    ax.plot(angles, entity_values, 'b-', linewidth=2, label='Entity-level')
    ax.fill(angles, entity_values, 'b', alpha=0.1)

    # Draw the token level metrics
    ax.plot(angles, token_values, 'r-', linewidth=2, label='Token-level')
    ax.fill(angles, token_values, 'r', alpha=0.1)

    # Add legend
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    plt.title('Radar Chart of Model Performance Metrics', size=15)
    plt.savefig('radar_metrics.png')
    plt.show()

# Plot radar chart
plot_radar_chart(test_metrics)

# Create a learning curve plot
plt.figure(figsize=(12, 8))

# Plot all metrics in one figure
plt.subplot(2, 1, 1)
plt.plot(range(1, epochs + 1), train_losses, 'b-', label='Training Loss')
plt.plot(range(1, epochs + 1), val_losses, 'r-', label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(range(1, epochs + 1), val_metrics_history['entity_f1'], 'g-', label='Entity F1')
plt.plot(range(1, epochs + 1), val_metrics_history['token_f1'], 'r-', label='Token F1')
plt.plot(range(1, epochs + 1), val_metrics_history['entity_accuracy'], 'b-', label='Entity Accuracy')
plt.plot(range(1, epochs + 1), val_metrics_history['token_accuracy'], 'y-', label='Token Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.title('Validation Metrics')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('learning_curves.png')
plt.show()

# Print final summary with recommendations
print("\n" + "="*50)
print("FINAL MODEL EVALUATION SUMMARY")
print("="*50)
print(f"Model: DistilBERT-base-cased for Named Entity Recognition")
print(f"Dataset: CoNLL-2003")
print(f"Training examples: {len(train_sentences)}")
print(f"Validation examples: {len(val_sentences)}")
print(f"Test examples: {len(test_sentences)}")
print(f"Number of entity classes: {num_labels}")
print("\nPERFORMANCE METRICS:")
print(f"Entity-level F1 Score: {test_metrics['entity_f1']:.4f}")
print(f"Entity-level Accuracy: {test_metrics['entity_accuracy']:.4f}")
print(f"Token-level F1 Score: {test_metrics['token_f1']:.4f}")
print(f"Token-level Accuracy: {test_metrics['token_accuracy']:.4f}")

# Identify best and worst performing entity types
per_class_data = [(label, metrics['f1-score'])
                 for label, metrics in test_metrics['report'].items()
                 if label not in ['micro avg', 'macro avg', 'weighted avg', 'accuracy']]

best_entity = max(per_class_data, key=lambda x: x[1])
worst_entity = min(per_class_data, key=lambda x: x[1])

print("\nBest performing entity type:")
print(f"  {best_entity[0]}: F1 Score = {best_entity[1]:.4f}")
print("Worst performing entity type:")
print(f"  {worst_entity[0]}: F1 Score = {worst_entity[1]:.4f}")

print("\nMODEL ARTIFACTS:")
print(f"- Saved model: {model_save_path}")
print("- Metrics CSV: metrics_summary.csv, per_class_metrics.csv")
print("- Visualizations: metrics_summary.png, confusion_matrix.png, etc.")
print("="*50)

import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from transformers import DistilBertTokenizerFast
from transformers import DistilBertForTokenClassification
from collections import defaultdict
import re
import html
from IPython.display import display, HTML

# Load the saved model and tokenizer
model_path = "distilbert_ner_model"
tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)
model = DistilBertForTokenClassification.from_pretrained(model_path)

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Put model in evaluation mode
model.eval()

# Get the label map from the model config
id2label = model.config.id2label
label2id = {v: k for k, v in id2label.items()}

# Map numeric labels to NER tags if needed
# This is needed because the model might have numeric labels like LABEL_0, LABEL_1, etc.
# You should replace these with your actual NER tags based on your training data
label_map = {
    "LABEL_0": "O",
    "LABEL_1": "B-PER",
    "LABEL_2": "I-PER",
    "LABEL_3": "B-ORG",
    "LABEL_4": "I-ORG",
    "LABEL_5": "B-LOC",
    "LABEL_6": "I-LOC",
    "LABEL_7": "B-MISC",
    "LABEL_8": "I-MISC"
}

# Function to perform NER on new text
def predict_entities(text, tokenizer, model, device):
    # Tokenize the text
    tokens = text.split()

    # Tokenize with the model's tokenizer
    inputs = tokenizer(
        tokens,
        is_split_into_words=True,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )

    # Move inputs to device
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)

    # Get predictions
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        predictions = torch.argmax(outputs.logits, dim=2)

    # Convert predictions to labels
    predicted_labels = []
    word_ids = inputs.word_ids()

    # Map predictions back to tokens
    previous_word_idx = None
    predicted_entities = []

    for idx, word_idx in enumerate(word_ids):
        if word_idx is None or word_idx == previous_word_idx:
            # Skip special tokens and continuation of words
            continue

        if word_idx < len(tokens):
            # Get the predicted label
            pred_label_id = predictions[0, idx].item()
            pred_label = id2label[pred_label_id]

            # Map to NER tag if needed
            if pred_label in label_map:
                pred_label = label_map[pred_label]

            predicted_labels.append(pred_label)

            # Store the token and its predicted label
            predicted_entities.append({
                "token": tokens[word_idx],
                "label": pred_label,
                "start": word_idx,
                "confidence": outputs.logits[0, idx, pred_label_id].item()
            })

        previous_word_idx = word_idx

    return predicted_entities, tokens

# Function to format entities for visualization
def format_for_visualization(entities, tokens):
    text = " ".join(tokens)

    # Calculate character offsets
    char_offsets = []
    current_offset = 0
    for token in tokens:
        char_offsets.append(current_offset)
        current_offset += len(token) + 1  # +1 for the space

    # Format entities for visualization
    formatted_entities = []
    current_entity = None

    for i, entity in enumerate(entities):
        if entity["label"] != "O":
            # Extract the entity type (remove B- or I- prefix)
            entity_type = entity["label"][2:] if entity["label"].startswith(("B-", "I-")) else entity["label"]

            # If it's a beginning of an entity or a new entity
            if entity["label"].startswith("B-") or (current_entity is None and entity["label"].startswith("I-")):
                if current_entity is not None:
                    formatted_entities.append(current_entity)

                # Start a new entity
                start_char = char_offsets[entity["start"]]
                end_char = start_char + len(entity["token"])

                current_entity = {
                    "start": start_char,
                    "end": end_char,
                    "label": entity_type,
                    "text": tokens[entity["start"]]
                }

            # If it's a continuation of an entity
            elif entity["label"].startswith("I-") and current_entity is not None:
                # Update the end of the current entity
                current_entity["end"] = char_offsets[entity["start"]] + len(entity["token"])
                current_entity["text"] += " " + tokens[entity["start"]]

    # Add the last entity if there is one
    if current_entity is not None:
        formatted_entities.append(current_entity)

    return {"text": text, "entities": formatted_entities}

# Custom HTML visualization function
def visualize_entities_html(doc):
    colors = {
        "PER": "#7aecec",
        "ORG": "#feca74",
        "LOC": "#ff9561",
        "MISC": "#bfeeb7"
    }

    text = doc["text"]
    entities = sorted(doc["entities"], key=lambda x: x["start"])

    # Create HTML with entity highlighting
    html_text = ""
    last_end = 0

    for entity in entities:
        # Add text before the entity
        html_text += html.escape(text[last_end:entity["start"]])

        # Get entity color
        entity_color = colors.get(entity["label"], "#dddddd")

        # Add the entity with highlighting
        entity_text = html.escape(text[entity["start"]:entity["end"]])
        html_text += f'<mark style="background-color: {entity_color}; border-radius: 0.25em; padding: 0.1em 0.2em; font-weight: bold;">{entity_text} <span style="font-size: 0.8em; font-weight: bold; vertical-align: middle; margin-left: 0.25em; color: #666;">{entity["label"]}</span></mark>'

        last_end = entity["end"]

    # Add any remaining text
    html_text += html.escape(text[last_end:])

    # Create the complete HTML
    complete_html = f"""
    <div style="font-family: 'Arial', sans-serif; line-height: 1.5; padding: 1em; max-width: 800px; border: 1px solid #ddd; border-radius: 0.5em;">
        <h3 style="margin-top: 0;">Named Entity Recognition</h3>
        <p>{html_text}</p>
        <div style="margin-top: 1em; font-size: 0.9em;">
            <h4 style="margin-top: 0;">Entity Types:</h4>
            <ul style="list-style-type: none; padding-left: 0; display: flex; flex-wrap: wrap; gap: 1em;">
    """

    # Add legend for entity types
    for label, color in colors.items():
        complete_html += f'<li style="display: flex; align-items: center;"><span style="display: inline-block; width: 1em; height: 1em; background-color: {color}; margin-right: 0.5em; border-radius: 0.25em;"></span>{label}</li>'

    complete_html += """
            </ul>
        </div>
    </div>
    """

    # Display the HTML
    display(HTML(complete_html))

# Function to print entities with confidence scores
def print_entities_with_confidence(entities, tokens):
    # Group entities by type
    entity_groups = defaultdict(list)

    current_entity = {"text": "", "type": "", "confidence": 0, "tokens": 0}

    for entity in entities:
        label = entity["label"]

        # Skip 'O' (Outside) labels
        if label == "O":
            if current_entity["text"]:
                # Add the completed entity to its group
                entity_type = current_entity["type"]
                avg_confidence = current_entity["confidence"] / current_entity["tokens"]
                entity_groups[entity_type].append({
                    "text": current_entity["text"].strip(),
                    "confidence": avg_confidence
                })
                current_entity = {"text": "", "type": "", "confidence": 0, "tokens": 0}
            continue

        # Extract entity type (remove B- or I- prefix)
        entity_type = label[2:] if label.startswith(("B-", "I-")) else label

        # If it's a beginning of a new entity
        if label.startswith("B-"):
            if current_entity["text"]:
                # Add the completed entity to its group
                prev_type = current_entity["type"]
                avg_confidence = current_entity["confidence"] / current_entity["tokens"]
                entity_groups[prev_type].append({
                    "text": current_entity["text"].strip(),
                    "confidence": avg_confidence
                })

            # Start a new entity
            current_entity = {
                "text": entity["token"] + " ",
                "type": entity_type,
                "confidence": entity["confidence"],
                "tokens": 1
            }

        # If it's a continuation of an entity
        elif label.startswith("I-") and current_entity["type"] == entity_type:
            current_entity["text"] += entity["token"] + " "
            current_entity["confidence"] += entity["confidence"]
            current_entity["tokens"] += 1

    # Add the last entity if there is one
    if current_entity["text"]:
        entity_type = current_entity["type"]
        avg_confidence = current_entity["confidence"] / current_entity["tokens"]
        entity_groups[entity_type].append({
            "text": current_entity["text"].strip(),
            "confidence": avg_confidence
        })

    # Print entities by type with confidence scores
    print("\nExtracted Entities:")
    print("==================")

    for entity_type, entities in entity_groups.items():
        print(f"\n{entity_type}:")
        for entity in sorted(entities, key=lambda x: x["confidence"], reverse=True):
            print(f"  - {entity['text']} (confidence: {entity['confidence']:.4f})")

    return entity_groups

# Interactive demo function
def interactive_demo():
    print("Named Entity Recognition Demo")
    print("============================")
    print("Type 'exit' to quit the demo")

    while True:
        text = input("\nEnter text to analyze: ")

        if text.lower() == 'exit':
            break

        # Predict entities
        entities, tokens = predict_entities(text, tokenizer, model, device)

        # Print entities with confidence scores
        entity_groups = print_entities_with_confidence(entities, tokens)

        # Visualize entities
        doc = format_for_visualization(entities, tokens)
        visualize_entities_html(doc)

        # Print entity statistics
        total_entities = sum(len(entities) for entities in entity_groups.values())
        print(f"\nStatistics: Found {total_entities} entities across {len(entity_groups)} types")

# Example usage
def example_usage():
    # Example texts
    examples = [
        "Apple Inc. is planning to open a new store in New York City next month.",
        "The European Union has imposed new sanctions on Russia over the Ukraine conflict.",
        "Michael Jordan scored 45 points when the Chicago Bulls defeated the Utah Jazz in the 1998 NBA Finals."
    ]

    for i, example in enumerate(examples):
        print(f"\nExample {i+1}:")
        print(f"Text: {example}")

        # Predict entities
        entities, tokens = predict_entities(example, tokenizer, model, device)

        # Print entities with confidence scores
        print_entities_with_confidence(entities, tokens)

        # Visualize entities
        doc = format_for_visualization(entities, tokens)
        visualize_entities_html(doc)

        print("\n" + "-"*50)

# Batch processing function
def batch_process_texts(texts):
    """
    Process a batch of texts and return all entities

    Args:
        texts: List of text strings to process

    Returns:
        List of dictionaries with text and extracted entities
    """
    results = []

    for text in texts:
        # Predict entities
        entities, tokens = predict_entities(text, tokenizer, model, device)

        # Format for output
        doc = format_for_visualization(entities, tokens)

        # Group entities by type
        entity_groups = defaultdict(list)
        for ent in doc["entities"]:
            entity_groups[ent["label"]].append(ent["text"])

        # Add to results
        results.append({
            "text": text,
            "entities": entity_groups
        })

    return results

# Function to save entities to file
def save_entities_to_file(results, output_file="extracted_entities.txt"):
    with open(output_file, "w", encoding="utf-8") as f:
        for i, result in enumerate(results):
            f.write(f"Text {i+1}: {result['text']}\n")
            f.write("Entities:\n")

            for entity_type, entities in result["entities"].items():
                f.write(f"  {entity_type}:\n")
                for entity in entities:
                    f.write(f"    - {entity}\n")

            f.write("\n" + "-"*50 + "\n")

    print(f"Entities saved to {output_file}")

# Main function
if __name__ == "__main__":
    print("NER Model loaded successfully!")
    print(f"Model has {len(id2label)} labels: {', '.join(id2label.values())}")

    # Choose one of the following:

    # 1. Run the interactive demo
    interactive_demo()

    # 2. Run example usage
    # example_usage()

    # 3. Process a batch of texts
    # texts = [
    #     "Apple CEO Tim Cook announced a new iPhone model during a press conference in Cupertino, California yesterday.",
    #     "Microsoft Corporation has acquired GitHub for $7.5 billion in Microsoft stock. Satya Nadella, CEO of Microsoft, said that GitHub will continue to operate independently with Nat Friedman as CEO. The deal is expected to close by the end of 2018.",
    #     "The Los Angeles Lakers defeated the Miami Heat 106-93 in Game 6 of the NBA Finals to win their 17th championship. LeBron James was named Finals MVP after averaging 29.8 points, 11.8 rebounds, and 8.5 assists during the series."
    # ]
    # results = batch_process_texts(texts)
    # save_entities_to_file(results)

